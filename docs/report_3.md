# Отчет по семинару № 3
Исследование поведения серверов flask и gunicorn под разными видами нагрузки.  

### Введение
Для тяжелых моделей предиктивной аналитики возможно два варианта деплоя. 
Первый вариант - запускать модели на своем сервере. 
Этот вариант имеет очевидный недостаток. 
Если у вас очень тяжелая модель, то пользователи вашего сервиса должны будут долго ждать ответа.  
Даже самый мощный компьютер имеет предел вычислительной мощности. 
Поэтому если вашим сервисом будут пользоваться несколько пользователей одновременно, придется настраивать собственный вычислительный кластер. 

Второй вариант - использовать специальные сервисы, например:  
- TensorFlow Serving
- AWS SageMaker
- Yandex DataSphere
- Google Vertex AI

В этом случае вычислительная нагрузка снимается с вашего сервера. 
Но за каждый запрос к стороннему сервису нужно платить, как деньги, так и временем на обработку запросов. 

### Метод исследования
В файле `src/utils.py` определены три функции, которые эмулируют три варианта решения задачи `predict` :
- `predict_io_bounded(area)` - соответсвует второму варианту, запрос к стороннему сервису заменяет `time.sleep(1)`. 
Это соответствует задержке в 1 секунду, которая нужна для обмена информацией со сторонним сервисом. 
При этом вычислительная нагрузка на наш сервер не создается, процесс просто спит. 
- `predict_cpu_bounded(area, n)` - соответствует первому варианту, предикту на собственном сервере. 
Параметр `n` позволяет регулировать нагрузку, на самом деле это просто вычисление среднего арифметического линейного массива. 
При достаточно больших `n` сервер будет выдавать ошибку из-за нехватки памяти. 
Необходимо эмпирическим путем определить это значение. 
- `predict_cpu_multithread(area, n)` - тоже соответствует первому варианту, но используется оптимизированный код на numpy. 
Необходимо также эмпирическим путем определить критическое значение `n` и сравнить его с предыдущим. 

Для запуска сервиса доступно два варианта: 
- `python src/predict_app.py` - сервер, предназначенный для разработки. 
- `gunicorn src.predict_app:app` - сервер, предназначенный для непрерывной работы в продакшн. 

Нагрузка создается файлом `test/test_parallel.py`.  

**Задача**: запустить 6 (шесть) возможных вариантов сочетаний серверов и функций под нагрузкой в 10 запросов. 

Результат запуска должен быть сохранен в логи, например с помощью перенаправления вывода:  
`python test/test_parallel.py > log/test_np_flask.txt` 
Обратите внимание, файлы должны иметь расширение txt, а значит не игнорятся гитом и должны быть запушены в мастере.  

### Результат и обсуждение
1) При запуске predict_cpu_multithread на dev-сервере flask с n=20_000_000 получен [результат](docs/log/flask_cpu_multithread.txt). 
Все запросы обрабатываются одновременно, в среднем за 1,25 секунды. 
На продакшене это неприемлимо, потому что разработческий сервер Flask работает на однопоточной основе и не предназначен для высоконагруженных условий. . В то время как для небольшого числа запросов Flask может обеспечить стабильное время ответа, при увеличении нагрузки его производительность может значительно снизиться 

2) При запуске predict_cpu_multithread на prod-сервере gunicorn с n=450_000_000  получен [результат](docs/log/gunicorn_cpu_multithread.txt). 
Все запросы обрабатываются последовательно, в среднем за 17 секунд. 
В продакшен-среде использование Gunicorn является более предпочтительным, так как он способен обрабатывать запросы в параллельном режиме. Время обработки запросов при этом будет зависеть от текущей загрузки сервера и степени конкуренции за вычислительные ресурсы.

3) При запуске predict_cpu_bounded на dev-сервере Flask с n=10_000_000 получен [результат](docs/log/Flask_cpu_bounded.txt).  Все запросы обрабатываются одновременно, в среднем за 7 секунд.Такой подход не подходит для продакшен-среды, поскольку сервер Flask в режиме разработки не предназначен для обработки большого количества запросов и работает на однопоточной основе.

4) При запуске predict_cpu_bounded на prod-сервере gunicorn с n= 100_000_000 получен [результат](docs/log/gunicorn_cpu_bounded.txt). Все запросы обрабатываются последовательно, в среднем за 39 секунды. На продакшене это предпочтительно по сравнению с dev-сервером Flask. Такой вариант является более предпочтительным для продакшена,но при больших значениях сервер не справляется с нагрузкой.

5) При запуске predict_io_bounded на dev-сервере flask с эмуляцией задержки ввода-вывода (1 сек) получен результат [результат](docs/log/flask_io_bounded.txt). Все запросы обрабатываются одновременно, в среднем за 1.1 секунд. На продакшене это неприемлимо, потому что Flask в режиме разработки использует однопоточный сервер, который не оптимизирован для работы в условиях высокой нагрузки.

6)При запуске predict_io_bounded на prod-сервере gunicorn с эмуляцией задержки ввода-вывода (1 сек) получен результат[результат](docs/log/gunicorn_io_bounded.txt). Все запросы обрабатываются последовательно, в среднем за 5.6 секунды. На продакшене это предпочтительно, потому что Gunicorn обрабатывает запросы в параллельном режиме

### Разница между Development и Production серверами

Время выполнения запросов на Gunicorn может быть выше, чем на сервере Flask, однако это дает более точное понимание того, как приложение будет функционировать при реальных нагрузках.